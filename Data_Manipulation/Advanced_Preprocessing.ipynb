{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mne\n",
        "!pip install mne-connectivity\n",
        "!pip install pyedflib\n",
        "!pip install networkx\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "phtuiaJG1hWo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsuch_lmnRl6"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import pyedflib\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mne.preprocessing import ICA\n",
        "from tqdm.notebook import tqdm\n",
        "import networkx as nx\n",
        "from scipy import stats\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = \"/content/drive/My Drive/4106 Project/eeg-motor-movementimagery-dataset-1.0.0/files\"\n",
        "\n",
        "\n",
        "'''\n",
        "output_dir = \"/content/processed\"                               #change i guess!!!!!!\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#process uploaded files\n",
        "subject_pattern = re.compile(r'S(\\d+)')\n",
        "run_pattern = re.compile(r'R(\\d+)')\n",
        "uploaded_files = []\n",
        "\n",
        "for filename, content in uploaded.items():\n",
        "    if filename.endswith('.edf'):\n",
        "        # Extract subject ID if present in filename\n",
        "        subject_match = subject_pattern.search(filename)\n",
        "        subject_id = f\"S{subject_match.group(1)}\" if subject_match else \"unknown_subject\"\n",
        "\n",
        "        #create subject directory if it dne\n",
        "        subject_dir = os.path.join(base_dir, subject_id)\n",
        "        os.makedirs(subject_dir, exist_ok=True)\n",
        "\n",
        "        #save file to the appropriate location\n",
        "        file_path = os.path.join(subject_dir, filename)\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        #extract run number if present\n",
        "        run_match = run_pattern.search(filename)\n",
        "        run_info = f\"run {run_match.group(1)}\" if run_match else \"unknown run\"\n",
        "\n",
        "        print(f\"Saved {filename} ({subject_id}, {run_info}) to {file_path}\")\n",
        "        uploaded_files.append(file_path)\n",
        "    else:\n",
        "        print(f\"Skipping {filename} - not an EEG .edf file\")\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def bandpass_filter(data, sfreq, low=0.5, high=40.0, order=4):\n",
        "    nyq = 0.5 * sfreq\n",
        "    low /= nyq\n",
        "    high /= nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "'''\n",
        "\n",
        "def load_events(event_file):\n",
        "    events = []\n",
        "    with open(event_file, 'r', encoding='latin-1') as f: # Changed encoding to 'latin-1'\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 2:\n",
        "                try:\n",
        "                    timestamp = float(parts[0])\n",
        "                    label = parts[1]\n",
        "                    events.append((timestamp, label))\n",
        "                except Exception as e:\n",
        "                    print(f\"Skipping line due to error: {e}\")\n",
        "    return events\n",
        "\n",
        "#utility fxnsss\n",
        "\n",
        "def bandpass_filter(data, sfreq, low=0.8, high=30.0, order=4):\n",
        "    \"\"\"Apply bandpass filter to EEG data.\"\"\"\n",
        "    nyq = 0.5 * sfreq\n",
        "    low /= nyq\n",
        "    high /= nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def find_edf_files(base_path, pattern=\"**/*.edf\"):\n",
        "    \"\"\"Find all EDF files recursively in the base directory.\"\"\"\n",
        "    files = glob.glob(os.path.join(base_path, pattern), recursive=True)\n",
        "    return files\n",
        "\n",
        "def clean_channel_names(raw):\n",
        "    \"\"\"Clean channel names by removing trailing periods.\"\"\"\n",
        "    ch_names = raw.info['ch_names']\n",
        "    clean_names = {name: name.rstrip('.') for name in ch_names}\n",
        "    raw.rename_channels(clean_names)\n",
        "    return raw\n",
        "\n",
        "def extract_events(raw):\n",
        "    \"\"\"Extract events from raw data's annotations.\"\"\"\n",
        "    try:\n",
        "        events, event_id = mne.events_from_annotations(raw)\n",
        "        return events, event_id\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting events: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def apply_ica(raw, n_components=15, random_state=42):\n",
        "    \"\"\"Apply ICA without requiring EOG channels\"\"\"\n",
        "    #create copy of raw data\n",
        "    raw_copy = raw.copy()\n",
        "\n",
        "    #initialize ICA\n",
        "    ica = ICA(n_components=n_components, random_state=random_state, method='infomax')\n",
        "\n",
        "    #apply ICA!\n",
        "    ica.fit(raw_copy)\n",
        "\n",
        "    try:\n",
        "        #try EOG-based approach first\n",
        "        #nope\n",
        "        eog_indices, eog_scores = ica.find_bads_eog(raw_copy)\n",
        "        ica.exclude = eog_indices\n",
        "        print(f\"Found and excluded {len(eog_indices)} EOG-related components\")\n",
        "    except Exception as e:\n",
        "        #fall back to statistical approach\n",
        "        ica_sources = ica.get_sources(raw_copy).get_data()\n",
        "\n",
        "        #calculate metrics for each component\n",
        "        kurt = stats.kurtosis(ica_sources, axis=1)\n",
        "        var = np.var(ica_sources, axis=1)\n",
        "\n",
        "        #outliers\n",
        "        kurt_threshold = np.mean(kurt) + 2 * np.std(kurt)\n",
        "        var_threshold = np.mean(var) + 2 * np.std(var)\n",
        "\n",
        "        bad_idx = np.where((kurt > kurt_threshold) | (var > var_threshold))[0]\n",
        "\n",
        "        #3 components limit\n",
        "        if len(bad_idx) > 3:\n",
        "            component_scores = kurt[bad_idx] + var[bad_idx]/np.max(var)\n",
        "            worst_idx = bad_idx[np.argsort(component_scores)[-3:]]\n",
        "            ica.exclude = worst_idx\n",
        "        else:\n",
        "            ica.exclude = bad_idx\n",
        "\n",
        "        print(f\"No EOG channels found. Excluded {len(ica.exclude)} components based on statistics\")\n",
        "\n",
        "    #apply ICA!\n",
        "    raw_cleaned = raw_copy.copy()\n",
        "    ica.apply(raw_cleaned)\n",
        "\n",
        "    return raw_cleaned\n",
        "\n",
        "def calculate_connectivity(epochs, method='pli', fmin=8, fmax=30):\n",
        "    \"\"\"\n",
        "    Calculate connectivity matrix for EEG channels.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    epochs : mne.Epochs\n",
        "        Epoched EEG data\n",
        "    method : str\n",
        "        Connectivity method ('pli', 'plv', etc.)\n",
        "    fmin, fmax : float\n",
        "        Frequency band to use for connectivity\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    conn : ndarray\n",
        "        Connectivity matrix (n_channels x n_channels)\n",
        "    \"\"\"\n",
        "    #get data + info\n",
        "    sfreq = epochs.info['sfreq']\n",
        "    ch_names = epochs.ch_names\n",
        "\n",
        "\n",
        "    try:\n",
        "        #new API\n",
        "        from mne_connectivity import spectral_connectivity_epochs\n",
        "\n",
        "        #connectivity calc - new API\n",
        "        conn = spectral_connectivity_epochs(\n",
        "            epochs,\n",
        "            method=method,\n",
        "            mode='multitaper',\n",
        "            sfreq=sfreq,\n",
        "            fmin=fmin,\n",
        "            fmax=fmax,\n",
        "            faverage=True,\n",
        "            mt_adaptive=True,\n",
        "            n_jobs=1\n",
        "        )\n",
        "\n",
        "        #connectivity matrix\n",
        "        conn_matrix = conn.get_data(output='dense')[:, :, 0]  #1st (& only) frequency band\n",
        "\n",
        "    except (ImportError, ModuleNotFoundError):\n",
        "        try:\n",
        "            #older mne.connectivity\n",
        "            from mne.connectivity import spectral_connectivity\n",
        "\n",
        "            #connectivity calc- old API\n",
        "            data = epochs.get_data()\n",
        "            conn, freqs, times, n_epochs, n_tapers = spectral_connectivity(\n",
        "                data,\n",
        "                method=method,\n",
        "                mode='multitaper',\n",
        "                sfreq=sfreq,\n",
        "                fmin=fmin,\n",
        "                fmax=fmax,\n",
        "                faverage=True,\n",
        "                mt_adaptive=True,\n",
        "                n_jobs=1\n",
        "            )\n",
        "\n",
        "            #reshape to n_channels x n_channels\n",
        "            n_channels = len(ch_names)\n",
        "            conn_matrix = conn.reshape(n_channels, n_channels)\n",
        "\n",
        "        except (ImportError, ModuleNotFoundError):\n",
        "            raise ImportError(\"Neither mne_connectivity nor mne.connectivity is available. \"\n",
        "                             \"Please install with: pip install mne-connectivity\")\n",
        "\n",
        "    #ensure diagonal is zero\n",
        "    np.fill_diagonal(conn_matrix, 0)\n",
        "\n",
        "    return conn_matrix\n",
        "\n",
        "def create_graph_from_connectivity(connectivity, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Create a graph structure from connectivity matrix.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    connectivity : ndarray\n",
        "        Connectivity matrix (n_channels x n_channels)\n",
        "    threshold : float\n",
        "        Threshold for edge creation\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    edge_index : torch.Tensor\n",
        "        Edge indices for PyTorch Geometric\n",
        "    edge_attr : torch.Tensor\n",
        "        Edge attributes (connectivity values)\n",
        "    \"\"\"\n",
        "    #threshold\n",
        "    adj_matrix = connectivity.copy()\n",
        "    adj_matrix[adj_matrix < threshold] = 0\n",
        "\n",
        "    #create edge index & attributes\n",
        "    edges = []\n",
        "    edge_weights = []\n",
        "\n",
        "    for i in range(adj_matrix.shape[0]):\n",
        "        for j in range(adj_matrix.shape[1]):\n",
        "            if i != j and adj_matrix[i, j] > 0:\n",
        "                edges.append([i, j])\n",
        "                edge_weights.append(adj_matrix[i, j])\n",
        "\n",
        "    #check if there are any edges\n",
        "    if not edges:\n",
        "        #create a minimal edge index if no edges were created\n",
        "        edges = [[0, 1], [1, 0]]\n",
        "        edge_weights = [0.1, 0.1]\n",
        "\n",
        "    #convert to PyTorch tensors\n",
        "    edge_index = torch.tensor(edges).t().contiguous()\n",
        "    edge_attr = torch.tensor(edge_weights).float()\n",
        "\n",
        "    return edge_index, edge_attr\n",
        "\n",
        "def map_eeg_to_brodmann(channel_names):\n",
        "    \"\"\"\n",
        "    Map EEG channel names to Brodmann areas\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    channel_names : list of str\n",
        "        Channel names to map\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    ba_areas : list of str\n",
        "        Brodmann area labels for each channel\n",
        "    \"\"\"\n",
        "    # Dictionary mapping EEG channels to Brodmann areas\n",
        "    # Format: BA number + L/R/M (Left/Right/Midline)\n",
        "    eeg_to_ba = {\n",
        "        # Frontal\n",
        "        'fp1': '10L',    # Left anterior prefrontal\n",
        "        'fpz': '10M',    # Midline anterior prefrontal\n",
        "        'fp2': '10R',    # Right anterior prefrontal\n",
        "        'af7': '10L',    # Left anterior prefrontal\n",
        "        'af3': '09L',    # Left dorsolateral prefrontal\n",
        "        'afz': '09M',    # Midline dorsolateral prefrontal\n",
        "        'af4': '09R',    # Right dorsolateral prefrontal\n",
        "        'af8': '10R',    # Right anterior prefrontal\n",
        "        'f7': '45L',     # Left inferior frontal\n",
        "        'f5': '45L',     # Left inferior frontal\n",
        "        'f3': '08L',     # Left middle frontal\n",
        "        'f1': '08L',     # Left middle frontal\n",
        "        'fz': '08M',     # Midline frontal\n",
        "        'f2': '08R',     # Right middle frontal\n",
        "        'f4': '08R',     # Right middle frontal\n",
        "        'f6': '45R',     # Right inferior frontal\n",
        "        'f8': '45R',     # Right inferior frontal\n",
        "        # Frontal-Central\n",
        "        'ft7': '44L',    # Left inferior frontal\n",
        "        'fc5': '06L',    # Left premotor\n",
        "        'fc3': '06L',    # Left premotor\n",
        "        'fc1': '06L',    # Left premotor\n",
        "        'fcz': '06M',    # Supplementary motor area\n",
        "        'fc2': '06R',    # Right premotor\n",
        "        'fc4': '06R',    # Right premotor\n",
        "        'fc6': '06R',    # Right premotor\n",
        "        'ft8': '44R',    # Right inferior frontal\n",
        "        # Central\n",
        "        't7': '22L',     # Left superior temporal\n",
        "        'c5': '04L',     # Left primary motor\n",
        "        'c3': '04L',     # Left primary motor\n",
        "        'c1': '04L',     # Left primary motor\n",
        "        'cz': '04M',     # Midline motor\n",
        "        'c2': '04R',     # Right primary motor\n",
        "        'c4': '04R',     # Right primary motor\n",
        "        'c6': '04R',     # Right primary motor\n",
        "        't8': '22R',     # Right superior temporal\n",
        "        # Central-Parietal\n",
        "        'tp7': '21L',    # Left middle temporal\n",
        "        'cp5': '40L',    # Left supramarginal gyrus\n",
        "        'cp3': '40L',    # Left supramarginal gyrus\n",
        "        'cp1': '05L',    # Left somatosensory association\n",
        "        'cpz': '05M',    # Midline somatosensory association\n",
        "        'cp2': '05R',    # Right somatosensory association\n",
        "        'cp4': '40R',    # Right supramarginal gyrus\n",
        "        'cp6': '40R',    # Right supramarginal gyrus\n",
        "        'tp8': '21R',    # Right middle temporal\n",
        "        # Parietal\n",
        "        'p7': '39L',     # Left angular gyrus\n",
        "        'p5': '39L',     # Left angular gyrus\n",
        "        'p3': '07L',     # Left superior parietal\n",
        "        'p1': '07L',     # Left superior parietal\n",
        "        'pz': '07M',     # Midline parietal\n",
        "        'p2': '07R',     # Right superior parietal\n",
        "        'p4': '07R',     # Right superior parietal\n",
        "        'p6': '39R',     # Right angular gyrus\n",
        "        'p8': '39R',     # Right angular gyrus\n",
        "        # Parietal-Occipital\n",
        "        'po7': '19L',    # Left associative visual\n",
        "        'po3': '19L',    # Left associative visual\n",
        "        'poz': '19M',    # Midline associative visual\n",
        "        'po4': '19R',    # Right associative visual\n",
        "        'po8': '19R',    # Right associative visual\n",
        "        # Occipital\n",
        "        'o1': '17L',     # Left primary visual\n",
        "        'oz': '17M',     # Midline primary visual\n",
        "        'o2': '17R',     # Right primary visual\n",
        "    }\n",
        "\n",
        "    #lowercase & trailing periods\n",
        "    ba_areas = []\n",
        "    for ch in channel_names:\n",
        "        ch_clean = ch.lower().rstrip('.')\n",
        "        if ch_clean in eeg_to_ba:\n",
        "            ba_areas.append(eeg_to_ba[ch_clean])\n",
        "        else:\n",
        "            #if not found, keep original w ?\n",
        "            ba_areas.append(ch + '?')\n",
        "    return ba_areas\n",
        "\n",
        "def normalize_data(data, method='minmax'):\n",
        "    \"\"\"Normalize data using different methods.\"\"\"\n",
        "    if method == 'minmax':\n",
        "        return (data + 100) / 200  #similar to github\n",
        "    elif method == 'z':\n",
        "        return (data - np.mean(data)) / np.std(data)\n",
        "    elif method == 's':\n",
        "        #normalize along the time dimension for each channel\n",
        "        mean = np.mean(data, axis=-1, keepdims=True)\n",
        "        std = np.std(data, axis=-1, keepdims=True)\n",
        "        return (data - mean) / std\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "def visualize_connectivity(connectivity, ch_names, title=\"Brain Connectivity\"):\n",
        "    \"\"\"Visualize connectivity matrix as a heatmap and a graph.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Plot connectivity matrix\n",
        "    im = ax1.imshow(connectivity, cmap='viridis')\n",
        "    ax1.set_title(\"Connectivity Matrix\")\n",
        "\n",
        "    #only show subset of labels if too many channels\n",
        "    n = len(ch_names)\n",
        "    if n > 20:\n",
        "        step = n // 10\n",
        "        indices = np.arange(0, n, step)\n",
        "        ax1.set_xticks(indices)\n",
        "        ax1.set_yticks(indices)\n",
        "        ax1.set_xticklabels([ch_names[i] for i in indices], rotation=90)\n",
        "        ax1.set_yticklabels([ch_names[i] for i in indices])\n",
        "    else:\n",
        "        ax1.set_xticks(np.arange(len(ch_names)))\n",
        "        ax1.set_yticks(np.arange(len(ch_names)))\n",
        "        ax1.set_xticklabels(ch_names, rotation=90)\n",
        "        ax1.set_yticklabels(ch_names)\n",
        "\n",
        "    plt.colorbar(im, ax=ax1)\n",
        "\n",
        "    #plot graph\n",
        "    G = nx.from_numpy_array(connectivity)\n",
        "    pos = nx.circular_layout(G)\n",
        "    nx.draw_networkx(G, pos=pos, ax=ax2, with_labels=True,\n",
        "                    node_color='lightblue', node_size=500,\n",
        "                    font_size=10, font_weight='bold')\n",
        "    ax2.set_title(\"Connectivity Graph\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.subplots_adjust(top=0.85)\n",
        "    return fig"
      ],
      "metadata": {
        "id": "qrr0ZDLIuuXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edf_path = \"/content/drive/My Drive/4106 Project/eeg-motor-movementimagery-dataset-1.0.0/files/S001/S001R01.edf\"\n",
        "raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
        "\n",
        "# Print channel names\n",
        "print(\"Channels:\", raw.ch_names)\n",
        "\n",
        "# Print annotations (if present)\n",
        "print(\"Annotations:\", raw.annotations)\n",
        "\n",
        "# Convert annotations to events (if needed)\n",
        "events, event_id = mne.events_from_annotations(raw)\n",
        "print(\"Event dictionary:\", event_id)\n",
        "print(\"Events array:\\n\", events)"
      ],
      "metadata": {
        "id": "KpbpsZVE-jVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define valid motor imagery recordings and processing parameters\n",
        "#ICA\n",
        "#################modify to\n",
        "\n",
        "# Define valid motor imagery recordings + params\n",
        "valid_runs = ['R03', 'R04', 'R07', 'R08', 'R11', 'R12']\n",
        "tmin, tmax = 0.0, 2.0  # seconds for each epoch\n",
        "connectivity_methods = ['pli']  # Phase Lag Index- can add 'plv', 'wpli' etc.\n",
        "normalization_method = 'minmax' # Same as in official implementation\n",
        "\n",
        "\n",
        "batch_size = 10                  #try\n",
        "\n",
        "#base_dir = os.path.join(output_dir, \"/content/eeg_data\")\n",
        "\n",
        "base_dir = \"/content/drive/My Drive/4106 Project/eeg-motor-movementimagery-dataset-1.0.0/files\"\n",
        "output_dir = \"/content/drive/My Drive/4106 Project/eeg_data\"\n",
        "\n",
        "\n",
        "# Get all subject folders\n",
        "subject_folders = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
        "print(f\"Found {len(subject_folders)} subject folders\")\n",
        "\n",
        "# Process subjects in batches\n",
        "all_subjects_data = []\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for batch_idx in range(0, len(subject_folders), batch_size):\n",
        "    batch_folders = subject_folders[batch_idx:batch_idx+batch_size]\n",
        "    print(f\"Processing batch {batch_idx//batch_size + 1}/{(len(subject_folders)-1)//batch_size + 1}\")\n",
        "\n",
        "    for subject_folder in tqdm(batch_folders):\n",
        "        subject_path = os.path.join(base_dir, subject_folder)\n",
        "        subject_data = {\n",
        "            'subject_id': subject_folder,\n",
        "            'epochs_data': [],\n",
        "            'labels': [],\n",
        "            'connectivity': [],\n",
        "            'edge_indices': [],\n",
        "            'edge_attrs': []\n",
        "        }\n",
        "\n",
        "        # Find all EDF files for this subject\n",
        "        edf_files = [f for f in os.listdir(subject_path) if f.endswith(\".edf\")]\n",
        "\n",
        "        for edf_file in edf_files:\n",
        "            # Check if it's a valid run\n",
        "            if not any(run in edf_file for run in valid_runs):\n",
        "                continue  # Skip rest or irrelevant runs\n",
        "\n",
        "            edf_path = os.path.join(subject_path, edf_file)\n",
        "            try:\n",
        "                # Load and preprocess data\n",
        "                raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
        "                raw = clean_channel_names(raw)\n",
        "\n",
        "                # Extract events\n",
        "                events, event_id = extract_events(raw)\n",
        "                if events is None or len(events) == 0:\n",
        "                    print(f\"No events found in {edf_file}\")\n",
        "                    continue\n",
        "\n",
        "                # Pick EEG channels only\n",
        "                raw.pick_types(eeg=True)\n",
        "\n",
        "                # Apply bandpass filter\n",
        "                raw.filter(0.5, 40.0, fir_design='firwin')\n",
        "\n",
        "                # Apply ICA for artifact removal\n",
        "                raw_cleaned = apply_ica(raw)\n",
        "\n",
        "                # Create epochs\n",
        "                epochs = mne.Epochs(raw_cleaned, events, event_id=event_id,\n",
        "                                   tmin=tmin, tmax=tmax, baseline=None,\n",
        "                                   preload=True, verbose=False)\n",
        "\n",
        "                # Calculate connectivity for each method\n",
        "                for method in connectivity_methods:\n",
        "                    connectivity = calculate_connectivity(epochs, method=method)\n",
        "\n",
        "                    # Create graph structure\n",
        "                    edge_index, edge_attr = create_graph_from_connectivity(connectivity)\n",
        "\n",
        "                    # Normalize epochs data\n",
        "                    epochs_data = epochs.get_data()\n",
        "                    epochs_data_norm = normalize_data(epochs_data, method=normalization_method)\n",
        "\n",
        "                    # Store data\n",
        "                    subject_data['epochs_data'].append(epochs_data_norm)\n",
        "                    subject_data['labels'].append(epochs.events[:, -1])\n",
        "                    subject_data['connectivity'].append(connectivity)\n",
        "                    subject_data['edge_indices'].append(edge_index)\n",
        "                    subject_data['edge_attrs'].append(edge_attr)\n",
        "\n",
        "                print(f\"Processed {edf_file}: {epochs.get_data().shape[0]} samples\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process {edf_file}: {e}\")\n",
        "\n",
        "        # Add subject data if not empty\n",
        "        if len(subject_data['epochs_data']) > 0:\n",
        "            all_subjects_data.append(subject_data)\n",
        "\n",
        "    # Save batch data to avoid memory issues\n",
        "    batch_filename = os.path.join(output_dir, f\"batch_{batch_idx//batch_size + 1}.npz\")\n",
        "    np.savez_compressed(batch_filename, batch_data=all_subjects_data)\n",
        "    print(f\"Saved batch {batch_idx//batch_size + 1} with {len(all_subjects_data)} subjects\")\n",
        "\n",
        "print(f\"Processed {len(all_subjects_data)} subjects in total\")"
      ],
      "metadata": {
        "id": "oiunaH7F5cOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Split data by subjects to prevent data leakage\n",
        "np.random.seed(42)  # For reproducibility\n",
        "np.random.shuffle(all_subjects_data)\n",
        "\n",
        "# Calculate split indices\n",
        "train_ratio, val_ratio = 0.7, 0.15  # 70% train, 15% val, 15% test\n",
        "n_subjects = len(all_subjects_data)\n",
        "train_idx = int(n_subjects * train_ratio)\n",
        "val_idx = train_idx + int(n_subjects * val_ratio)\n",
        "\n",
        "# Split data\n",
        "train_data = all_subjects_data[:train_idx]\n",
        "val_data = all_subjects_data[train_idx:val_idx]\n",
        "test_data = all_subjects_data[val_idx:]\n",
        "\n",
        "print(f\"Split data: {len(train_data)} train, {len(val_data)} validation, {len(test_data)} test subjects\")"
      ],
      "metadata": {
        "id": "CQIOKyb468uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Prepare PyTorch Geometric data\n",
        "from torch_geometric.data import Data, Dataset\n",
        "\n",
        "def prepare_geometric_data(subject_data, label_map=None):\n",
        "    \"\"\"\n",
        "    Convert processed subject data to PyTorch Geometric format.\n",
        "\n",
        "    Args:\n",
        "        subject_data: Dictionary containing processed data for a subject\n",
        "        label_map: Dictionary mapping event IDs to class labels\n",
        "\n",
        "    Returns:\n",
        "        dataset_list: List of PyTorch Geometric Data objects\n",
        "    \"\"\"\n",
        "    dataset_list = []\n",
        "\n",
        "    # Use first connectivity matrix and edge index if multiple methods\n",
        "    connectivity_idx = 0\n",
        "\n",
        "    # For each recording session\n",
        "    for i in range(len(subject_data['epochs_data'])):\n",
        "        epochs = subject_data['epochs_data'][i]\n",
        "        labels = subject_data['labels'][i]\n",
        "        edge_index = subject_data['edge_indices'][i]\n",
        "        edge_attr = subject_data['edge_attrs'][i]\n",
        "\n",
        "        # For each epoch\n",
        "        for j in range(epochs.shape[0]):\n",
        "            # Prepare node features\n",
        "            x = torch.from_numpy(epochs[j]).float()  # (n_channels, n_times)\n",
        "\n",
        "            # Prepare label\n",
        "            label = int(labels[j])\n",
        "            y = torch.tensor([label]).long()\n",
        "\n",
        "            # Create s and d attributes\n",
        "            s = torch.tensor([label % 2]).long()  # Binary task (left vs right)\n",
        "            d = torch.tensor([label % 3]).long()  # 3-class task\n",
        "\n",
        "            # Create PyTorch Geometric Data object\n",
        "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, s=s, d=d)\n",
        "\n",
        "            dataset_list.append(data)\n",
        "\n",
        "    return dataset_list\n",
        "\n",
        "# Prepare train, validation, and test datasets\n",
        "train_dataset = []\n",
        "for subject in train_data:\n",
        "    train_dataset.extend(prepare_geometric_data(subject))\n",
        "\n",
        "val_dataset = []\n",
        "for subject in val_data:\n",
        "    val_dataset.extend(prepare_geometric_data(subject))\n",
        "\n",
        "test_dataset = []\n",
        "for subject in test_data:\n",
        "    test_dataset.extend(prepare_geometric_data(subject))\n",
        "\n",
        "print(f\"Prepared datasets: {len(train_dataset)} train, {len(val_dataset)} validation, {len(test_dataset)} test samples\")"
      ],
      "metadata": {
        "id": "5AEC8Gy67Ueq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.save datasets in format compatible with official implementation\n",
        "torch.save(train_dataset, os.path.join(output_dir, \"train_dataset_id\"))\n",
        "torch.save(val_dataset, os.path.join(output_dir, \"val_dataset_id\"))\n",
        "torch.save(test_dataset, os.path.join(output_dir, \"test_dataset_id\"))\n",
        "\n",
        "#save edge index for global use\n",
        "if len(train_dataset) > 0:\n",
        "    edge_index = train_dataset[0].edge_index.numpy()\n",
        "    np.savetxt(os.path.join(output_dir, \"edge_index.txt\"), edge_index)\n",
        "    print(\"Saved edge index to file\")\n",
        "\n",
        "    #save metadata if channels are available\n",
        "    try:\n",
        "        #get channel names from the first subject\n",
        "        n_nodes = train_dataset[0].x.shape[0]\n",
        "        ch_names = ['Ch' + str(i) for i in range(n_nodes)]\n",
        "        brodmann_labels = map_eeg_to_brodmann(ch_names)\n",
        "\n",
        "        #save metadata\n",
        "        metadata = {\n",
        "            'channel_names': ch_names,\n",
        "            'brodmann_labels': brodmann_labels\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(output_dir, \"metadata.pkl\"), 'wb') as f:\n",
        "            pickle.dump(metadata, f)\n",
        "\n",
        "        print(\"Saved metadata\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save metadata: {e}\")"
      ],
      "metadata": {
        "id": "dTqUsLC78wrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.zip file\n",
        "try:\n",
        "    zip_path = os.path.join(output_dir, \"eeg_gnn_data.zip\")\n",
        "    shutil.make_archive(os.path.splitext(zip_path)[0], 'zip', output_dir)\n",
        "    print(f\"Created zip archive at {zip_path}\")\n",
        "\n",
        "    #download in Colab ?\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(zip_path)\n",
        "    except:\n",
        "        pass\n",
        "except:\n",
        "    print(\"Failed to create zip archive\")"
      ],
      "metadata": {
        "id": "YhxBFVK09duU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9 visualizations:\n",
        "# Visualize a sample connectivity matrix and signals\n",
        "if len(train_dataset) > 0:\n",
        "    sample_data = train_dataset[0]\n",
        "    edge_index = sample_data.edge_index.numpy()\n",
        "    n_nodes = sample_data.x.shape[0]\n",
        "\n",
        "    # --- Build symmetric adjacency matrix ---\n",
        "    adj_matrix = np.zeros((n_nodes, n_nodes))\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        src, dst = edge_index[0, i], edge_index[1, i]\n",
        "        adj_matrix[src, dst] = 1\n",
        "        adj_matrix[dst, src] = 1  # ensure symmetry for visualization\n",
        "\n",
        "    # --- Channel names using 10-10 convention (basic guess) ---\n",
        "    # Replace this with actual channel labels if you have them\n",
        "    ch_names = [f'Ch{i}' for i in range(n_nodes)]\n",
        "\n",
        "    # --- Plot adjacency matrix ---\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(adj_matrix, cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=np.arange(n_nodes), labels=ch_names, rotation=90, fontsize=6)\n",
        "    plt.yticks(ticks=np.arange(n_nodes), labels=ch_names, fontsize=6)\n",
        "    plt.title(\"Connectivity Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"sample_connectivity_matrix.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot EEG signals ---\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(min(5, sample_data.x.shape[0])):\n",
        "        signal = sample_data.x[i].numpy()\n",
        "        signal = signal - np.mean(signal)  # remove DC offset\n",
        "        plt.plot(signal, label=f'Channel {i}')\n",
        "    plt.title(\"Sample EEG Signals (First 5 Channels)\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"sample_signals.png\"))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "WzZZgqxM-cAv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}