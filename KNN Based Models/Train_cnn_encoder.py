# -*- coding: utf-8 -*-
"""CNN encoder GNN decoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dAnQPgxx21dG9O_3jFmKPdSeua_dRRwW
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from sklearn.metrics.pairwise import cosine_similarity
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GCNConv, global_mean_pool


class ResidualBlock3Conv(nn.Module):
    def __init__(self, channels, kernel_size=5):
        super().__init__()
        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size // 2)
        self.bn1 = nn.BatchNorm1d(channels)

        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size // 2)
        self.bn2 = nn.BatchNorm1d(channels)

        self.conv3 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size // 2)
        self.bn3 = nn.BatchNorm1d(channels)

    def forward(self, x):
        residual = x

        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))

        out += residual  # üîÅ Skip connection
        return F.relu(out)

class EEG_Conv1D(nn.Module):
    def __init__(self, input_channels, output_features):
        super(EEG_Conv1D, self).__init__()

        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=5, stride=1, padding=2)
        self.bn1 = nn.BatchNorm1d(64)
        self.relu1 = nn.ReLU()
        self.res1 = ResidualBlock3Conv(64)

        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=2)
        self.bn2 = nn.BatchNorm1d(128)
        self.relu2 = nn.ReLU()
        self.res2 = ResidualBlock3Conv(128, kernel_size=3)

        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=2)
        self.bn3 = nn.BatchNorm1d(256)
        self.relu3 = nn.ReLU()
        self.res3 = ResidualBlock3Conv(256, kernel_size=3)

        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=2)
        self.bn4 = nn.BatchNorm1d(512)
        self.relu4 = nn.ReLU()

        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.dropout = nn.Dropout(p=0.5)
        self.fc = nn.Linear(512, 128)

        self.dropout2 = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(128, output_features)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x = F.normalize(x, p=1, dim=1)  # L2 norm
        x = self.res1(x)
        x = F.normalize(x, p=1, dim=1)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu2(x)
        x = F.normalize(x, p=1, dim=1)
        x = self.res2(x)
        x = F.normalize(x, p=1, dim=1)
        x = self.res2(x)
        x = F.normalize(x, p=1, dim=1)
        x = self.res2(x)
        x = F.normalize(x, p=1, dim=1)

        x = self.conv3(x)
        x = self.bn3(x)
        x = self.relu3(x)
        x = F.normalize(x, p=1, dim=1)
        x = self.res3(x)
        x = F.normalize(x, p=1, dim=1)
        x = self.res3(x)
        x = F.normalize(x, p=1, dim=1)
        x = self.res3(x)
        x = F.normalize(x, p=1, dim=1)

        x = self.conv4(x)
        x = self.bn4(x)
        x = self.relu4(x)
        x = F.normalize(x, p=1, dim=1)

        x = self.global_pool(x)
        x = x.squeeze(-1)
        x = self.dropout(x)
        x = self.fc(x)
        x = F.normalize(x, p=1, dim=1)  # L2 normalize after FC
        x = self.dropout2(x)
        x = self.fc2(x)
        x = F.normalize(x, p=1, dim=1)  # Final output normalization

        return x

class EEG_GNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(EEG_GNN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.classifier = nn.Linear(hidden_dim, num_classes)

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = global_mean_pool(x, batch)  # Pool to graph-level
        return self.classifier(x)

# 3. k-NN edge builder
def build_knn_edge_index(features, k=5):
    sim_matrix = cosine_similarity(features)
    edge_index = []
    for i in range(sim_matrix.shape[0]):
        top_k = sim_matrix[i].argsort()[-(k+1):]
        for j in top_k:
            if i != j:
                edge_index.append([i, j])
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    return edge_index

from torch_geometric.data import Data, DataLoader
import torch
import numpy as np

def make_graph(x_np, y_np):
    graphs = []
    for i in range(len(x_np)):
        x = torch.tensor(x_np[i], dtype=torch.float32)  # shape: (C, T)
        y = torch.tensor(y_np[i], dtype=torch.long)

        # Connect timepoints as a chain or self-loop
        num_nodes = x.shape[0]  # Get the number of nodes

        # Create edge indices using torch.arange and reshape
        # to avoid exceeding the number of nodes
        edge_index = torch.arange(num_nodes, dtype=torch.long)
        # Change: Create edge indices within the valid node range
        edge_index = torch.stack([edge_index[:-1], edge_index[1:]], dim=0)
        # Change: Clip indices to ensure they are within bounds
        edge_index = torch.clamp(edge_index, 0, num_nodes - 1)

        edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)  # bidirectional

        # Reshape x for Conv1D: (C, T)
        x = x.unsqueeze(0)  # Add a batch dimension

        graphs.append(Data(x=x, edge_index=edge_index, y=y))
    return graphs

class EEGGraphDataset(torch.utils.data.Dataset):
    def __init__(self, eeg_array, label_array, encoder, k=5):
        """
        eeg_array: [num_samples, num_nodes, 64, 321]
        label_array: [num_samples]
        """
        self.eeg_array = eeg_array
        self.label_array = label_array
        self.encoder = encoder.eval()
        self.k = k

    def __len__(self):
        return len(self.eeg_array)

    def __getitem__(self, idx):
        # Get 1 EEG sample: shape [num_nodes, 64, 321]
        eeg_sample = self.eeg_array[idx]
        label = torch.tensor(self.label_array[idx], dtype=torch.long)

        # Convert to tensor: [num_nodes, 64, 321]
        eeg_tensor = torch.tensor(eeg_sample, dtype=torch.float32)

        # DEBUG: Print shape
        # print("eeg_tensor shape BEFORE encoder:", eeg_tensor.shape)

        # encoder expects: [B, C, T] ‚Üí [num_nodes, 64, 321]
        with torch.no_grad():
            features = self.encoder(eeg_tensor.to(self.encoder.fc.weight.device))  # [num_nodes, feat_dim]

        # Build edge index using k-NN on node features
        edge_index = knn_graph(features, k=self.k, loop=False)

        # Return Data object
        return Data(x=features, edge_index=edge_index, y=label.unsqueeze(0))

data = np.load("eeg_dataset.npz")
X = data['X']
y = data['y']
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.long)
X_tensor = X_tensor.permute(0,1,2)

print(X_tensor.shape)

from torch_geometric.nn import knn_graph
import torch
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.data import TensorDataset, DataLoader, random_split

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
X_tensor = torch.tensor(X_tensor)  # ensure it's a PyTorch tensor
y_tensor = torch.tensor(y_tensor)

dataset = TensorDataset(X_tensor, y_tensor)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Create data loaders
batch_size = 2048
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# === 4. Model, optimizer, loss ===
in_channels = X_tensor.shape[1]  # number of EEG channels (features per node)
num_classes = len(np.unique(y_tensor)) + 1

# model = EEG_GNN(input_dim=128, hidden_dim=64, num_classes=num_classes).to(device)
model = EEG_Conv1D(input_channels = in_channels, output_features=num_classes).to(device)

import torch.optim as optim
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []

num_epochs = 100
for epoch in range(num_epochs):
    # Training
    model.train()
    total_train_loss = 0
    correct_train = 0
    total_train = 0

    for X, y in train_loader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        outputs = model(X)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()

        total_train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct_train += (predicted == y).sum().item()
        total_train += y.size(0)

    avg_train_loss = total_train_loss / len(train_loader)
    train_acc = correct_train / total_train

    # Validation
    model.eval()
    total_val_loss = 0
    correct_val = 0
    total_val = 0

    with torch.no_grad():
        for X, y in val_loader:
            X, y = X.to(device), y.to(device)
            outputs = model(X)
            loss = criterion(outputs, y)
            total_val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_val += (predicted == y).sum().item()
            total_val += y.size(0)

    avg_val_loss = total_val_loss / len(val_loader)
    val_acc = correct_val / total_val

    train_losses.append(avg_train_loss)
    val_losses.append(avg_val_loss)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)

    print(f"Epoch {epoch+1}/{num_epochs} - "
          f"Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f} | "
          f"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}")

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Val Loss")
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label="Train Acc")
plt.plot(val_accuracies, label="Val Acc")
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.tight_layout()
plt.show()

torch.save(model.state_dict(), 'cnn_head.pth')